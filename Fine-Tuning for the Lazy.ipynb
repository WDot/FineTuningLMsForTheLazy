{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce096f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "MODEL = 'gpt2'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL).cuda()\n",
    "generator = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer,device=0)\n",
    "\n",
    "%set_env TOKENIZERS_PARALLELISM =true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d11df6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "DATA_PATH = 'TODO PUT UR FILE HERE'\n",
    "\n",
    "#Creates pairs of sequences (X,Y) XY is a coherent string\n",
    "class LMData(Dataset):\n",
    "    def __init__(self,textPath,tokenizer,kernel_size=256,stride=16):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        with open(textPath) as x: self.text = x.read()\n",
    "        self.tokens = []\n",
    "        for i in range(self.get_length(self.text,self.kernel_size,self.kernel_size)):\n",
    "            cur_text = self.get_independent_window(self.text,i,self.kernel_size,self.kernel_size)\n",
    "            cur_tokens = tokenizer(cur_text)\n",
    "            self.tokens.extend(cur_tokens['input_ids'])\n",
    "        self.tokens = np.array(self.tokens)\n",
    "        \n",
    "    def get_independent_window(self,iterable,i,kernel_size,stride):\n",
    "        return iterable[stride*i:(stride*i+kernel_size)]\n",
    "        \n",
    "    def get_length(self,iterable,kernel_size,stride):\n",
    "        return ((len(iterable) - kernel_size)// stride)\n",
    "        \n",
    "    \n",
    "    def __getitem__(self,i):\n",
    "        cur_tokens = self.get_independent_window(self.tokens,i,self.kernel_size+1,self.stride)\n",
    "        X = cur_tokens[:self.kernel_size]\n",
    "        Y = cur_tokens[1:(self.kernel_size+1)]\n",
    "        return(X,Y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        #Usually + 1, but we want to generate two sequences every time!\n",
    "        return self.get_length(self.tokens,self.kernel_size+1,self.stride)\n",
    "    \n",
    "dataset = LMData(DATA_PATH,tokenizer)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098ef191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "\n",
    "def _init_(exp_name):\n",
    "    if not os.path.exists('checkpoints'):\n",
    "        os.makedirs('checkpoints')\n",
    "    if not os.path.exists('checkpoints/'+ exp_name):\n",
    "        os.makedirs('checkpoints/'+ exp_name)\n",
    "    if not os.path.exists('checkpoints/'+ exp_name+'/'+'models'):\n",
    "        os.makedirs('checkpoints/'+ exp_name+'/'+'models')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d61dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import sys\n",
    "import time\n",
    "\n",
    "\n",
    "def train(exp_name):\n",
    "    train_dataset = LMData(DATA_PATH,tokenizer)\n",
    "    train_loader = DataLoader(train_dataset,num_workers=8,batch_size=10, shuffle=True, drop_last=True)\n",
    "\n",
    "    #model = DistilBertForMaskedLM.from_pretrained(MODEL).cuda()\n",
    "    device = 0\n",
    "    \n",
    "    sys.stdout.flush()\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    \n",
    "    LR = 0.001\n",
    "    MOMENTUM = 0.9\n",
    "    EPOCHS = 100\n",
    "\n",
    "    opt = optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=1e-4)\n",
    "\n",
    "    scheduler = CosineAnnealingLR(opt, EPOCHS, eta_min=LR)\n",
    "    \n",
    "    #criterion = cal_loss\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    best_loss = 100.0\n",
    "    for epoch in range(EPOCHS):\n",
    "        t = time.time()\n",
    "\n",
    "        losses = []\n",
    "        ####################\n",
    "        # Train\n",
    "        ####################\n",
    "        train_loss = 0.0\n",
    "        count = 0.0\n",
    "        model.train()\n",
    "        train_pred = []\n",
    "        train_true = []\n",
    "        for before, after in train_loader:\n",
    "            before, after = before.to(device), after.to(device)\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                output = model(input_ids=before,labels=after)\n",
    "                logits = output['logits']\n",
    "                loss = output['loss']\n",
    "            losses.append(loss.detach().cpu().numpy())\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            #opt.step()\n",
    "            #break;\n",
    "            \n",
    "            \n",
    "            count += 1\n",
    "            if count % 1000 == 0:\n",
    "                print('Loss: {0}'.format(np.mean(losses)))\n",
    "                sys.stdout.flush()\n",
    "        cur_loss =np.mean(losses)\n",
    "        print('Epoch {0} Loss: {1}'.format(epoch,cur_loss))\n",
    "        sys.stdout.flush()\n",
    "            \n",
    "        scheduler.step()\n",
    "\n",
    "        if best_loss >= cur_loss:\n",
    "            best_loss = cur_loss\n",
    "            torch.save(model.state_dict(), 'checkpoints/%s/models/model.t7' % exp_name)\n",
    "            \n",
    "        #VALIDATE\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for i in range(3):\n",
    "                #outputs = model.generate(' ',do_sample=True, max_length=60, pad_token_id=50256)\n",
    "                print(generator(' ',pad_token_id=50256, num_return_sequences=1))\n",
    "                #print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "        #break\n",
    "        print('Epoch {0} Time: {1}'.format(epoch,time.time() - t))\n",
    "            \n",
    "EXP_NAME = 'robocanon1'\n",
    "\n",
    "_init_(EXP_NAME)\n",
    "train(EXP_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0168fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = 'abcdefghijk'\n",
    "K = 3\n",
    "S = 3\n",
    "\n",
    "def get_independent_window(iterable,i,kernel_size,stride):\n",
    "        return iterable[stride*i:(stride*i+kernel_size)]\n",
    "        \n",
    "cur_text = get_independent_window(TEXT,1,K+1,S)\n",
    "print(cur_text[:K])\n",
    "print(cur_text[1:(K+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053c9d48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
